# S3 Storage Module - White Label Ready

## Overview

The S3 Storage module creates a flexible, white label S3-based storage infrastructure with intelligent tiering, lifecycle policies, and Athena-optimized partitioning. Designed for multi-tenant SaaS platforms, analytics services, media platforms, and enterprise solutions.

## Features

- **White Label Ready**: Configurable naming, use cases, and multi-tenant support
- **Intelligent Tiering**: Automatic cost optimization by moving data between storage classes
- **Lifecycle Policies**: Automated data archival and cleanup
- **Athena Partitioning**: Year/month/day/hour partition structure for efficient querying
- **Multi-Tenant Support**: Primary and secondary data prefixes for isolation
- **Cross-Account Access**: Support for partner and client account integration
- **Flexible IAM Roles**: Read-only, admin, and custom access patterns
- **Security**: Encryption, versioning, and least-privilege IAM roles
- **Monitoring**: CloudWatch logging and access tracking

## Architecture

```
White Label S3 Bucket Structure:
├── {primary_data_prefix}/           # Configurable: "data/", "uploads/", "analytics/"
│   └── year=YYYY/month=MM/day=DD/hour=HH/
│       ├── events.json
│       └── metrics.parquet
├── {secondary_prefixes}/            # Multi-tenant: "tenant-a/", "partner-data/"
│   └── year=YYYY/month=MM/day=DD/hour=HH/
├── processed/
│   └── year=YYYY/month=MM/day=DD/hour=HH/
│       └── aggregated_data.parquet
└── {temp_prefixes}/                # Configurable: "temp/", "staging/", "processing/"
    └── files.json (auto-deleted based on expiration_days)
```

### White Label Use Cases
- **Analytics Platform**: `analytics-data-lake` with event/metric prefixes
- **Media Platform**: `media-storage` with image/video/document prefixes  
- **SaaS Platform**: `tenant-data` with per-client isolation
- **Backup Service**: `enterprise-backup` with daily/weekly/monthly prefixes
- **Data Exchange**: `cross-account-sharing` with partner access

## Resource Naming

All S3 resources include the environment name for clear identification and multi-environment support:

**Resource Naming Pattern:**
- S3 Bucket: `${instance_name}-${environment}-${bucket_name_suffix}-${random_hex}`
- IAM Roles: `${instance_name}-${environment}-s3-${use_case}-{role_type}-role`
- IAM Policies: `${instance_name}-${environment}-s3-${use_case}-{role_type}-policy`

**Examples:**
```
Staging Environment (environment = "staging"):
- S3 Bucket: mht-api-staging-raw-data-collection-abc123
- Access Role: mht-api-staging-s3-raw-data-collection-access-role
- Admin Role: mht-api-staging-s3-raw-data-collection-admin-role

Production Environment (environment = "production"):
- S3 Bucket: mht-api-production-production-data-def456
- Access Role: mht-api-production-s3-production-analytics-access-role
- Admin Role: mht-api-production-s3-production-analytics-admin-role
```

This naming convention ensures complete resource separation between environments while maintaining clear identification of purpose and ownership.

## Usage

### Basic White Label Configuration

```hcl
module "s3_storage" {
  source = "./modules/s3"
  
  instance_name      = "client-platform"
  environment        = var.environment  # "staging" or "production"
  bucket_name_suffix = "analytics"
  use_case          = "data-analytics"
  
  tags = {
    Environment = var.environment
    Client      = "ClientA"
  }
}
```

### Multi-Tenant SaaS Configuration

```hcl
module "s3_storage" {
  source = "./modules/s3"
  
  instance_name      = "saas-platform"
  bucket_name_suffix = "tenant-data" 
  use_case          = "multi-tenant-saas"
  
  # Multi-tenant data organization
  primary_data_prefix     = "tenant-data/"
  secondary_data_prefixes = [
    "tenant-a/",
    "tenant-b/",
    "shared/",
    "exports/"
  ]
  
  # Cross-account access for enterprise clients
  trusted_accounts = ["111122223333", "444455556666"]
  
  # Multiple access patterns
  create_read_only_role = true    # For tenant read access
  create_admin_role     = true    # For platform admin
  
  # Flexible temporary storage
  temp_prefixes = {
    "exports" = {
      prefix          = "exports"
      expiration_days = 7
    }
    "imports" = {
      prefix          = "imports" 
      expiration_days = 3
    }
  }
  
  tags = {
    Environment = "production"
    Platform    = "SaaS"
    MultiTenant = "true"
  }
}
```

### Enterprise Media Platform

```hcl
module "media_storage" {
  source = "./modules/s3"
  
  instance_name      = "media-platform"
  bucket_name_suffix = "media-storage"
  use_case          = "media-content-delivery"
  
  # Media-specific organization
  primary_data_prefix     = "uploads/"
  secondary_data_prefixes = [
    "images/",
    "videos/", 
    "documents/",
    "thumbnails/"
  ]
  
  # Media-optimized lifecycle (longer retention)
  lifecycle_transitions = [
    {
      days          = 60    # Keep media hot longer
      storage_class = "STANDARD_IA"
    },
    {
      days          = 180
      storage_class = "GLACIER"
    }
  ]
  
  # Media processing temporary storage
  temp_prefixes = {
    "upload-queue" = {
      prefix          = "upload-queue"
      expiration_days = 1
    }
    "processing" = {
      prefix          = "processing"
      expiration_days = 7
    }
  }
  
  # Enhanced security for media
  versioning_enabled = true
  kms_key_id        = "arn:aws:kms:region:account:key/media-key-id"
  
  tags = {
    Environment = "production"
    Platform    = "Media"
    CDN         = "enabled"
  }
}
```

## Data Ingestion Examples

### Using AWS CLI

```bash
# Upload raw data with proper partitioning
aws s3 cp data.json s3://bucket-name/data/year=2024/month=01/day=15/hour=14/

# Upload processed data
aws s3 cp processed.parquet s3://bucket-name/processed/year=2024/month=01/day=15/hour=14/
```

### Using Python with boto3

```python
import boto3
from datetime import datetime

s3 = boto3.client('s3')
bucket_name = 'your-bucket-name'

# Get current timestamp for partitioning
now = datetime.now()
partition_path = f"data/year={now.year}/month={now.month:02d}/day={now.day:02d}/hour={now.hour:02d}/"

# Upload data with proper partitioning
s3.put_object(
    Bucket=bucket_name,
    Key=f"{partition_path}events_{now.strftime('%Y%m%d_%H%M%S')}.json",
    Body=json.dumps(data),
    ContentType='application/json'
)
```

### Using EC2 Instance with IAM Role

```bash
# On EC2 instance with attached IAM instance profile
curl -X POST http://169.254.169.254/latest/meta-data/iam/security-credentials/role-name

# Upload using the instance profile credentials
aws s3 cp /tmp/data.json s3://bucket-name/data/year=2024/month=01/day=15/hour=14/
```

## Athena Integration

### Creating External Table

```sql
CREATE EXTERNAL TABLE raw_events (
  event_id string,
  timestamp string,
  user_id string,
  event_type string,
  properties map<string,string>
)
PARTITIONED BY (
  year int,
  month int,
  day int,
  hour int
)
STORED AS JSON
LOCATION 's3://your-bucket-name/data/'
TBLPROPERTIES ('has_encrypted_data'='false');
```

### Adding Partitions

```sql
-- Add specific partition
ALTER TABLE raw_events ADD PARTITION (year=2024, month=1, day=15, hour=14)
LOCATION 's3://your-bucket-name/data/year=2024/month=01/day=15/hour=14/';

-- Discover partitions automatically
MSCK REPAIR TABLE raw_events;
```

### Querying Data

```sql
-- Query specific time range with partition pruning
SELECT event_type, COUNT(*) as event_count
FROM raw_events
WHERE year = 2024 
  AND month = 1 
  AND day = 15 
  AND hour BETWEEN 10 AND 14
GROUP BY event_type;
```

## Cost Optimization

### Intelligent Tiering Benefits

- **Standard to IA**: Objects not accessed for 30 days automatically move to Infrequent Access
- **IA to Archive**: Objects not accessed for 90 days move to Archive Access tier
- **Archive to Deep Archive**: Objects not accessed for 180 days move to Deep Archive

### Lifecycle Policy Benefits

- **Automatic Transitions**: Predictable cost reduction through storage class transitions
- **Version Management**: Old versions automatically cleaned up
- **Incomplete Upload Cleanup**: Failed uploads don't consume storage

### Cost Monitoring

```bash
# Monitor storage costs by storage class
aws s3api get-bucket-metrics-configuration --bucket your-bucket-name
```

## Security Features

### Encryption
- Server-side encryption with AES-256 (default) or KMS
- Bucket key enabled for cost optimization
- Encrypted in transit (HTTPS only)

### Access Control
- IAM roles with least-privilege permissions
- Bucket policies denying insecure connections
- Public access blocked by default

### Monitoring
- CloudWatch access logs
- S3 bucket notifications for automation
- Versioning for data protection

## Variables

| Name | Description | Type | Default |
|------|-------------|------|---------|
| `instance_name` | Name prefix for resources | `string` | - |
| `enable_intelligent_tiering` | Enable S3 Intelligent Tiering | `bool` | `true` |
| `enable_lifecycle_policy` | Enable lifecycle policy | `bool` | `true` |
| `versioning_enabled` | Enable bucket versioning | `bool` | `true` |
| `data_prefix` | Prefix for data objects | `string` | `"data/"` |
| `create_partition_examples` | Create example partitions | `bool` | `true` |

## Outputs

| Name | Description |
|------|-------------|
| `bucket_id` | S3 bucket ID |
| `bucket_arn` | S3 bucket ARN |
| `data_ingestion_role_arn` | IAM role ARN for data ingestion |
| `athena_partition_example` | Example partition structure |
| `cost_optimization_features` | Enabled cost optimization features |

## Best Practices

1. **Partitioning**: Always use the year/month/day/hour structure for optimal query performance
2. **File Formats**: Use columnar formats (Parquet, ORC) for better compression and query speed
3. **File Sizes**: Keep files between 128MB-1GB for optimal query performance
4. **Naming**: Use consistent naming conventions for easy data discovery
5. **Monitoring**: Set up CloudWatch alarms for storage costs and access patterns

## Troubleshooting

### Common Issues

1. **Partition Not Found**: Ensure partition path matches exactly `year=YYYY/month=MM/day=DD/hour=HH/`
2. **Access Denied**: Verify IAM role has correct permissions and is attached to resource
3. **High Costs**: Check if intelligent tiering and lifecycle policies are properly configured
4. **Query Performance**: Ensure proper partitioning and file formats are used

## White Label Examples

For complete real-world configuration examples, see:
- **[White Label S3 Examples](../white-label/s3-examples.md)** - 6 comprehensive use case configurations including:
  - Analytics Data Lake
  - Media Storage Platform
  - Multi-Tenant SaaS
  - Enterprise Backup Solution
  - Cross-Account Data Sharing
  - Development/Staging Environment

Each example includes complete tfvars configurations, best practices, and deployment guidance.